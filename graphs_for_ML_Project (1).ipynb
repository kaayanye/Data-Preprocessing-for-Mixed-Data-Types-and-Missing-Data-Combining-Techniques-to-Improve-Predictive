{"cells":[{"cell_type":"markdown","source":["## Before you start, upload the train.csv and test.csv files"],"metadata":{"id":"EbWgz8bhTjRf"},"id":"EbWgz8bhTjRf"},{"cell_type":"code","execution_count":null,"id":"395928e9-17dd-48d3-850d-4012d7e64230","metadata":{"id":"395928e9-17dd-48d3-850d-4012d7e64230"},"outputs":[],"source":["# Install necessary packages\n","\n","# pip install seaborn"]},{"cell_type":"code","execution_count":null,"id":"f46d7dae-bc7f-4e22-aac2-d3e3d6fa4c69","metadata":{"id":"f46d7dae-bc7f-4e22-aac2-d3e3d6fa4c69"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import sklearn\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","from scipy import stats\n","from math import ceil\n","from scipy.stats import probplot\n","from sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score\n","from sklearn.metrics import make_scorer\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n","from sklearn.svm import LinearSVR, SVR\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"code","execution_count":null,"id":"fb7ce0a2-2071-4b2d-960f-be0f00afa11a","metadata":{"id":"fb7ce0a2-2071-4b2d-960f-be0f00afa11a"},"outputs":[],"source":["init_df = pd.read_csv('train.csv')"]},{"cell_type":"code","execution_count":null,"id":"d85c12ba-39c7-4e08-929c-438f8401ff62","metadata":{"id":"d85c12ba-39c7-4e08-929c-438f8401ff62"},"outputs":[],"source":["display(init_df)\n","\n","# Show counts of NaN values in columns with NaN values\n","for i, col in zip(init_df.isna().sum(), init_df.columns):\n","  if i > 0:\n","      print('{:<20}{}'.format(col, i))"]},{"cell_type":"markdown","source":["As seen in the dataframe there are several types of data, with a mix of categorical and numerical data. There are also multiple values missing, or listed as NaN. This means we are going to have to preprocess the data before we can conduct our analyses and models."],"metadata":{"id":"2CM3LHSdNitT"},"id":"2CM3LHSdNitT"},{"cell_type":"markdown","id":"8a913e28-a771-4ab4-916c-2c9fc6ab8ab2","metadata":{"id":"8a913e28-a771-4ab4-916c-2c9fc6ab8ab2"},"source":["##Data Preprocessing and Exploratory Data Analysis"]},{"cell_type":"markdown","source":["### Processing missing values"],"metadata":{"id":"ig_v1FV5NfkQ"},"id":"ig_v1FV5NfkQ"},{"cell_type":"markdown","source":["First split the dataset into a training and validation dataset *before* any of the missing values are processed.\n","\n","We use an 80% training and 20% validation split. Test data is already in a separate file."],"metadata":{"id":"aEPmnCpxwslU"},"id":"aEPmnCpxwslU"},{"cell_type":"code","source":["# Validation set is a sample of 20%. Seed is used to get consistent results while testing\n","validate = init_df.sample(frac=0.2,random_state=200)\n","# training is set acquired by dropping the validation set\n","train = init_df.drop(validate.index)\n","\n","# df is a copy of the training set, so we can keep training set unmodified\n","df = train.copy()\n"],"metadata":{"id":"ZRYRe7tcwsrO"},"id":"ZRYRe7tcwsrO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dividing the data into their respective levels of measurement"],"metadata":{"id":"hb08Jk4gXrjY"},"id":"hb08Jk4gXrjY"},{"cell_type":"code","execution_count":null,"id":"eadcbdb9-951f-41ae-84bf-ce9febb94c78","metadata":{"id":"eadcbdb9-951f-41ae-84bf-ce9febb94c78"},"outputs":[],"source":["# First divide data into the numerical or categorical, so we can preprocess it\n","\n","# List of nominal and ordinal columns extracted from data_description.txt\n","nom_col = ['MSSubClass', 'MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1',\n","           'Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd',\n","           'BsmtFinType1','BsmtFinType2','Electrical','FireplaceQu','PavedDrive','Fence','SaleCondition',\n","           'MasVnrType','Foundation','Heating','CentralAir','GarageType','SaleType','MiscFeature']\n","\n","ord_col = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n","           'HeatingQC','KitchenQual','Functional',\n","           'GarageFinish','GarageQual','GarageCond','PoolQC','OverallQual','OverallCond']\n","\n","# List of all columns\n","allcat = list(df.columns[1:])\n","\n","# List of numerical columns\n","num_col = [x for x in allcat if x not in nom_col+ord_col]\n"]},{"cell_type":"code","source":["# Check the discrete numerical features:\n","for i in df[num_col]:\n","    cats = list(set(list(df[i])))\n","    if len(cats) < 20:\n","        print(\"{:<20}{}\".format(i, cats))\n","print()\n","pool = df[['PoolQC','PoolArea','SalePrice']].fillna(999)\n","for q, a, p in zip(pool.PoolQC, pool.PoolArea, pool.SalePrice):\n","    if q != 999:\n","        print(q, a, p)\n","print()\n","print(pool['PoolArea'].corr(pool['SalePrice']))"],"metadata":{"id":"jecrG9ilF6Xc"},"id":"jecrG9ilF6Xc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, features such as FullBath and such have very limited values. Hence, we will use these as ordinal data instead.\n","We also have numbers for the months and years sold, which can dictate the value, but is not related to how high the number is-- Hence, these should actually be considered nominal.\n","\n","For PoolArea we also only have 6 houses in the dataset with pools, and we see that the correlation between PoolArea and SalePrice is 0.092, it is almost none. So, we will drop all pool-related columns from our dataset, since there are so few values, and, at least the area, has very little to do with the sale price.\n","\n","So, in the df, we remove the Pool-related columns, we move MoSold and YrSold to the list of nominal columns, and we move everything else (FullBath, GarageCars, FirePlaces...) to the ordinal columns.\n","The remaining columns in the num_col will then actually be the numerical columns."],"metadata":{"id":"2Hr-MEiLLFNe"},"id":"2Hr-MEiLLFNe"},{"cell_type":"code","source":["remove = ['PoolQC' , 'PoolArea']\n","ordinals = []\n","nominals = ['MoSold' , 'YrSold']\n","\n","# Check the discrete numerical features:\n","for i in df[num_col]:\n","    cats = list(set(list(df[i])))\n","    if len(cats) < 20:\n","        if i not in remove+nominals:\n","            ordinals.append(i)\n","\n","# Fix the lists\n","num_col = [x for x in num_col if x not in remove+ordinals+nominals]\n","ord_col = ord_col+ordinals\n","ord_col.remove('PoolQC')\n","nom_col = nom_col+nominals\n","\n","# Fix the df:\n","df.drop(remove, axis=1, inplace=True)"],"metadata":{"id":"W_loiEykVtXJ"},"id":"W_loiEykVtXJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#missing data\n","total = df.isnull().sum().sort_values(ascending=False)\n","percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)*100\n","missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n","missing_data.head(20)"],"metadata":{"id":"r_B4Utn-4Zz6"},"id":"r_B4Utn-4Zz6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By looking into the data_description.txt, we see that a lot of the missing' NaN' values are not actually missing measurements, but rather meant to imply that the measured object was not present. For example, MiscFeatures is listed NaN for properties that did not have MiscFeatures\n","\n","The solution to this is to find all categories that use NaN in this way, and substitute NaN for some other value-- For strings, this could be 'none', and for integers, this could be 0."],"metadata":{"id":"NKRS_AVgUhTe"},"id":"NKRS_AVgUhTe"},{"cell_type":"code","source":["# See the columns that contain NaN values:\n","for i in df.columns:\n","    if df[i].isna().sum() != 0:\n","        print('{:<20}{}'.format(i, df[i].isna().sum()))\n"],"metadata":{"id":"HQ6k-wwvYQyh"},"id":"HQ6k-wwvYQyh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of these, drop electrical because there's only one property, drop MasVnrType because NaN means the value were not recorded. MasVnrArea NaNs are also dropped for the same reason. GarageYrBuilt is set to same year as the house (0 would be too far from all the other values), and the rest will have all 'NaN' replaced with 0 for ordinal columns and 'None' for nominal columns.\n","\n","LotFrontage is the only continuous numerical value that is truly missing, based on our assumption that all properties should have a connected street. This will be estimated using multivariate imputation."],"metadata":{"id":"uSxMaTKcdGnT"},"id":"uSxMaTKcdGnT"},{"cell_type":"code","source":["# Drop the na rows from the df\n","df.dropna(subset = ['Electrical','MasVnrType','MasVnrArea'], inplace=True)"],"metadata":{"id":"VnyK7ruofAQ7"},"id":"VnyK7ruofAQ7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def impute(input):\n","\n","    #imp = IterativeImputer(max_iter=10, random_state=0)\n","    imp = IterativeImputer(n_nearest_features=None, imputation_order='ascending')\n","    imp.fit(input)\n","    output = pd.DataFrame(imp.transform(df), columns = df.columns)\n","    return output\n","\n","def fixnan(input):\n","    # Fix the garage year built\n","    input.loc[input.GarageYrBlt.isnull(),'GarageYrBlt'] = input.loc[input.GarageYrBlt.isnull(),'YearBuilt']\n","\n","    # Fix MasVnrArea as it is for some reason an object type\n","    input.MasVnrArea = input.MasVnrArea.astype(str).astype(float)\n","\n","    # Fill NaNs\n","    for col in nom_col:\n","        input[col].fillna('None',inplace=True)\n","    for col in ord_col:\n","        input[col].fillna('None',inplace=True)\n","\n","    # Remove LotFrontage since we don't want to fill this\n","    col_num_nan = num_col\n","    try: col_num_nan.remove('LotFrontage')\n","    except: pass\n","\n","    for col in col_num_nan:\n","        input[col].fillna('0',inplace=True)\n","    return input\n","\n","fixnan(df)\n","\n","# Encode all nominal data as numerical with one-hot encoding to create regression\n","def onehot(data):\n","    output = pd.get_dummies(data=data, drop_first=True)\n","    output = pd.get_dummies(data=output, columns=['MSSubClass'])\n","    return output\n","\n","# Encode ordinal data with arbitrary number sequences\n","def labenc(data):\n","    le = preprocessing.LabelEncoder()\n","    output = data.apply(preprocessing.LabelEncoder().fit_transform)\n","    return output\n","\n","df[ord_col] = labenc(df[ord_col])\n","\n","df = onehot(df)\n","\n","#df = impute(df)"],"metadata":{"id":"b6gynW0sgaJ9"},"id":"b6gynW0sgaJ9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","# Find categorical columns with intentional NaN values (see data_description.txt)\n","col_cat_nan = ['MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n","           'GarageQual','GarageCond','GarageFinish','GarageType',\n","           'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n","\n","# Find numerical columns with intentional NaN values (see data_description.txt)\n","col_num_nan = ['MasVnrArea','BsmtFullBath','BsmtHalfBath','BsmtFinSF1',\n","               'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageArea','GarageCars']\n","\n","# Use pandas built-in 'fillna' function to remove nan values:\n","def fillnan(input):\n","    for col in nom_col:\n","        input[col].fillna('None',inplace=True)\n","\n","    for col in col_num_nan:\n","        input[col].fillna('0',inplace=True)\n","    return input\n","\n","# Apply to df:\n","df = fillnan(df)\n","\n","# Check which columns still contain NaN values:\n","#df.loc[:, df.isna().any()]\n","\n","# There are 3. The first, GarageYrBlt, we will solve by just using the year\n","# that the house was built: YearBlt.\n","# The second, Electrical, only contains one single house where it was not\n","# measured. This can be checked with:\n","#   list(set(list(df['Electrical'])))\n","#   df['Electrical'].isna().value_counts()\n","# This is solved by dropping that single row from our dataset.\n","# The third is LotFrontage, a numerical measurement. There are too many here\n","# to simply drop the rows, so instead we will use the average of this value\n","# for these houses.\n","\n","def multivar_impute(input):\n","\n","  imp = IterativeImputer(max_iter=10, random_state=0)\n","  imp.fit(input)\n","\n","  output = imp.transform(input)\n","  return output\n","\n","\n","\n","# Perform on df:\n","df = fixnan(df)\n","\n","# Perform on validate:\n","vali = fillnan(validate)\n","val = fixnan(vali)\n","\n","\"\"\""],"metadata":{"id":"xDgQgpnBOG9-"},"id":"xDgQgpnBOG9-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# Encode all nominal data as numerical with one-hot encoding to create regression\n","def onehot(data):\n","    output = pd.get_dummies(data=data, drop_first=True)\n","    output = pd.get_dummies(data=output, columns=['MSSubClass'])\n","    return output\n","\n","# Apply to df:\n","df_nom = onehot(nom_col)\n","\n","# Encode ordinal data with arbitrary number sequences\n","def labenc(data):\n","    le = preprocessing.LabelEncoder()\n","    output = data.apply(preprocessing.LabelEncoder().fit_transform)\n","    return output\n","\n","# Apply to df:\n","df_ord = labenc(df[ord_col])\n","df = pd.concat([df_ord, df_nom, df_num], axis=1)\n","\n","\n","\n","# Impute missing LotFrontage values with multivariate imputing\n","imputed = multivar_impute(df)\n","df = pd.DataFrame(imputed)\n","\"\"\""],"metadata":{"id":"QfZ--DK6FyFP"},"id":"QfZ--DK6FyFP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify that there are no NaN values in df or val:\n","print(df.isnull().any().sum())\n","#print(val.isnull().any().sum())\n","for i in df.columns:\n","    if df[i].isnull().any().sum() == 1:\n","        print(i)\n","df"],"metadata":{"id":"3xMJgyWf4yjW"},"id":"3xMJgyWf4yjW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["skewlist = []\n","for i in df[num_col]:\n","    if abs(df[i].skew()) > 0.5:\n","        skewlist.append(i)\n","print(skewlist)"],"metadata":{"id":"pM60Zklhs-hE"},"id":"pM60Zklhs-hE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the distributions of the skewed categories before and after using log transformation\n","# to see which ones work well for log transformation\n","for i, v in enumerate(df[skewlist]):\n","    print(v)\n","    gby = np.log1p(df[v])\n","    fig = plt.figure(figsize=(15,8))\n","    ax = fig.add_subplot(222)\n","    ax.set_title(v)\n","    res = probplot(gby, plot=ax)\n","    ax2 = fig.add_subplot(221)\n","    ax.set_title(v)\n","    res = probplot(df[v], plot=ax2)\n","    plt.show()\n","    print()"],"metadata":{"id":"D0TKa30neI4p"},"id":"D0TKa30neI4p","execution_count":null,"outputs":[]},{"cell_type":"code","source":["logtransf = ['1stFlrSF','GrLivArea','SalePrice']\n","\n","for i, v in enumerate(df[logtransf]):\n","  tmp = np.log(df[v])\n","  df[v] = tmp"],"metadata":{"id":"PYnyN5MAy6Oh"},"id":"PYnyN5MAy6Oh","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8d814702-e1c2-4b86-ac29-d58af37f9d70","metadata":{"id":"8d814702-e1c2-4b86-ac29-d58af37f9d70"},"source":["### Plotting a correlation matrix of the numerical data"]},{"cell_type":"code","execution_count":null,"id":"bcb9b27e-1435-4752-b561-b2857167fcec","metadata":{"id":"bcb9b27e-1435-4752-b561-b2857167fcec"},"outputs":[],"source":["# Plot numerical data in a correlation matrix\n","df_num = df[num_col]\n","# Create correlation between these columns of data\n","corr = df_num.corr()\n","\n","plt.figure(figsize=(25,15))\n","\n","# Create array of 0s in same shape as corr.shape\n","mask = np.zeros_like(corr)\n","# Take upper triangle of the shape of 0s\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Change mask and correlation to remove empty rows\n","mask = mask[1:, :-1]\n","corr = corr.iloc[1:,:-1].copy()\n","\n","# Choose a diverging palette for the r values (0 should be white)\n","cmap = sns.diverging_palette(220, 20, as_cmap=True)\n","\n","# Plot the heatmap\n","sns.heatmap(corr,         # Create heatmap of the df.corr created above\n","            mask=mask,    # Use the mask to not show repeated boxes\n","            annot=True,   # Annotate the squares with the r value\n","            fmt=\".2f\",    # Use 2 decimals\n","            annot_kws={\"size\": 8}, \n","            cmap=cmap,    # cmap is the diverging one created above\n","            square=True,  # Square boxes look better\n","            cbar_kws={\"shrink\": .8}, # Shrink color bar to center it\n","            vmin=-1, vmax=1) # Max and Min values for the colors\n","\n","plt.show()"]},{"cell_type":"markdown","id":"56d6c75c-0517-4e90-8aaf-158725560e99","metadata":{"id":"56d6c75c-0517-4e90-8aaf-158725560e99"},"source":["It's pretty clear that there's some correlations, but most of the high correlations are pretty obvious or not very useful, for example 'GarageCars' and 'GarageArea' or 'YearBuilt' and 'GarageYrBlt'.\n","\n","A few useful correlations are present, like GrLiveArea and SalePrice.### Plotting a correlation matrix of the ordinal data"]},{"cell_type":"markdown","id":"f0e94ef9-58ce-4aa0-9d31-48d460bee866","metadata":{"id":"f0e94ef9-58ce-4aa0-9d31-48d460bee866"},"source":["### Plotting a correlation matrix of the ordinal data"]},{"cell_type":"code","execution_count":null,"id":"4bd065f4-edb2-47b6-8ded-d2b6df417369","metadata":{"id":"4bd065f4-edb2-47b6-8ded-d2b6df417369"},"outputs":[],"source":["# Encode all ordinal data as numerical to create regression\n","df_ord = df[ord_col]\n","# Create correlation between these columns of data\n","ord_corr = df_ord.corr()\n","\n","plt.figure(figsize=(25,15))\n","\n","# Create array of 0s in same shape as corr.shape\n","mask = np.zeros_like(ord_corr)\n","# Take upper triangle of the shape of 0s\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Change mask and correlation to remove empty rows\n","mask = mask[1:, :-1]\n","ord_corr = ord_corr.iloc[1:,:-1].copy()\n","\n","# Choose a diverging palette for the r values (0 should be white)\n","cmap = sns.diverging_palette(220, 20, as_cmap=True)\n","\n","# Plot the heatmap\n","sns.heatmap(ord_corr,         # Create heatmap of the df.corr created above\n","            mask=mask,    # Use the mask to not show repeated boxes\n","            annot=True,   # Annotate the squares with the r value\n","            fmt=\".2f\",    # Use 2 decimals\n","            annot_kws={\"size\": 8}, \n","            cmap=cmap,    # cmap is the diverging one created above\n","            square=True,  # Square boxes look better\n","            cbar_kws={\"shrink\": .8}, # Shrink color bar to center it\n","            vmin=-1, vmax=1) # Max and Min values for the colors\n","\n","plt.show()"]},{"cell_type":"markdown","id":"2500e2cc-f660-4eb1-8aac-5696e82fcd54","metadata":{"id":"2500e2cc-f660-4eb1-8aac-5696e82fcd54"},"source":["### Computing correlation coefficients for one-hot encoded nominal data"]},{"cell_type":"code","execution_count":null,"id":"14076b13-073f-4741-9f12-799ae75fd329","metadata":{"id":"14076b13-073f-4741-9f12-799ae75fd329"},"outputs":[],"source":["# Encode all nominal data as numerical with one-hot encoding to create regression\n","#df[nom_col]['SalePrice'] = df['SalePrice']\n","\n","corr_sp = df.corr()\n","print(corr_sp['SalePrice'].abs().sort_values(ascending=False)[1:])\n","\n","topten = ['OverallQual','GrLivArea','GarageArea','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','Fireplaces','MasVnrArea','BsmtFinSF1']\n","topfive = ['OverallQual','GrLivArea','GarageCars','GarageArea','1stFlrSF']"]},{"cell_type":"code","source":["sns.set_context(\"paper\", rc={\"font.size\":14,\"axes.titlesize\":14,\"axes.labelsize\":14})\n","# Encode all ordinal data as numerical to create regression\n","df_five = df[topfive]\n","# Create correlation between these columns of data\n","ord_corr = df_five.corr()\n","\n","plt.figure(figsize=(3.1,3))\n","\n","# Create array of 0s in same shape as corr.shape\n","mask = np.zeros_like(ord_corr)\n","# Take upper triangle of the shape of 0s\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Change mask and correlation to remove empty rows\n","mask = mask[1:, :-1]\n","ord_corr = ord_corr.iloc[1:,:-1].copy()\n","\n","# Choose a diverging palette for the r values (0 should be white)\n","cmap = sns.diverging_palette(220, 20, as_cmap=True)\n","\n","# Plot the heatmap\n","sns.heatmap(ord_corr,         # Create heatmap of the df.corr created above\n","            mask=mask,    # Use the mask to not show repeated boxes\n","            annot=True,   # Annotate the squares with the r value\n","            fmt=\".2f\",    # Use 2 decimals\n","            annot_kws={\"size\": 10}, \n","            cmap=cmap,    # cmap is the diverging one created above\n","            square=True,  # Square boxes look better\n","            cbar_kws={\"shrink\": .8}, # Shrink color bar to center it\n","            vmin=-1, vmax=1).set_title(\"Five Features With Highest Correlation to Sale Price\",fontsize=12) # Max and Min values for the colors\n","\n","plt.show()"],"metadata":{"id":"GFv4wVElhIxQ"},"id":"GFv4wVElhIxQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode all ordinal data as numerical to create regression\n","df_ten = df[topten]\n","# Create correlation between these columns of data\n","ord_corr = df_ten.corr()\n","\n","plt.figure(figsize=(3.1,3))\n","\n","# Create array of 0s in same shape as corr.shape\n","mask = np.zeros_like(ord_corr)\n","# Take upper triangle of the shape of 0s\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Change mask and correlation to remove empty rows\n","mask = mask[1:, :-1]\n","ord_corr = ord_corr.iloc[1:,:-1].copy()\n","\n","# Choose a diverging palette for the r values (0 should be white)\n","cmap = sns.diverging_palette(220, 20, as_cmap=True)\n","\n","# Plot the heatmap\n","sns.heatmap(ord_corr,         # Create heatmap of the df.corr created above\n","            mask=mask,    # Use the mask to not show repeated boxes\n","            annot=True,   # Annotate the squares with the r value\n","            fmt=\".2f\",    # Use 2 decimals\n","            annot_kws={\"size\": 8}, \n","            cmap=cmap,    # cmap is the diverging one created above\n","            square=True,  # Square boxes look better\n","            cbar_kws={\"shrink\": .8}, # Shrink color bar to center it\n","            vmin=-1, vmax=1).set_title(\"Ten Features With Highest Correlation to Sale Price\") # Max and Min values for the colors\n","\n","plt.show()"],"metadata":{"id":"S8gFh_N3jRTR"},"id":"S8gFh_N3jRTR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exploring the target variable: SalePrice"],"metadata":{"id":"rtMY1gdWQWJd"},"id":"rtMY1gdWQWJd"},{"cell_type":"markdown","source":["### Plotting the distribution of the sale prices\n","First we plot a histogram and a normal qq-plot to asses the distribution"],"metadata":{"id":"w0__iC46n-bU"},"id":"w0__iC46n-bU"},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(16,5))\n","ax = sns.histplot(data=df.SalePrice, kde=True, ax=axes[0])\n","ax1 = stats.probplot(df.SalePrice.dropna(), plot=axes[1])\n","plt.show()"],"metadata":{"id":"w5582V2pQciN"},"id":"w5582V2pQciN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is pretty clear from the above charts that SalePrice is *not* normally distributed with a clear right-skew.\n","\n","This is not necessarily a problem in itself, but for some of our models this is not ideal, and a distribution that is closer to normal is preferred.\n","Fortunately, it does look like it conforms to a log-normal distribution, so there is a very simple method called log-transform that should transform it into something closer to a normal distribution."],"metadata":{"id":"aNkRgRY4oV79"},"id":"aNkRgRY4oV79"},{"cell_type":"code","source":["# Make a copy of dataframe\n","df_log = df.copy()\n","# The SalePrice column in the copy becomes the log of the SalePrice in df\n","df_log.SalePrice = np.log(df.SalePrice)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(16,5))\n","ax = sns.histplot(data=df_log.SalePrice, kde=True, ax=axes[0])\n","ax1 = stats.probplot(df_log.SalePrice.dropna(), plot=axes[1])\n","plt.show()"],"metadata":{"id":"HCIBJCt0plsX"},"id":"HCIBJCt0plsX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The new, 'standard' df will have:\n","# - one-hot encoded nominal categories\n","# - numerically encoded ordinal categories\n","# - All missing values filled in with the various methods performed so far\n","# - Log-transformed SalePrice to conform to normality\n","# df = pd.concat([df_log.SalePrice, df_ord.drop(['SalePrice'], axis=1), df_nom.drop(['SalePrice'], axis=1), df_num.drop(['SalePrice'], axis=1)], axis=1)"],"metadata":{"id":"E8MOxoe4r5yK"},"id":"E8MOxoe4r5yK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Which categories best predict the sale price?\n","corr_sp = df.corr()\n","print(corr_sp['SalePrice'].abs().sort_values(ascending=False)[1:])"],"metadata":{"id":"WI6AYcGhLgrx"},"id":"WI6AYcGhLgrx","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2107d0cb-e7cd-4fa7-a4dc-bc8c3be3f65a","metadata":{"id":"2107d0cb-e7cd-4fa7-a4dc-bc8c3be3f65a"},"source":["## Linear Regression Model\n","\n","### Using all processed data"]},{"cell_type":"code","source":["print(df[topten].isna().sum())\n","df = df.dropna(subset=topten)\n","#print(df.isna().sum())"],"metadata":{"id":"Z2xeFYkO8ru0"},"id":"Z2xeFYkO8ru0","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1d940533-ef9c-49a7-b821-a8c2722a8c29","metadata":{"id":"1d940533-ef9c-49a7-b821-a8c2722a8c29"},"outputs":[],"source":["# Linear regression using all data\n","\n","x_col = df.drop(['SalePrice'], axis=1)\n","X = x_col.values.reshape(-1, len(x_col.columns))\n","y = df['SalePrice'].values\n","\n","reg = LinearRegression().fit(X, y)\n","\n","print(reg.score(X, y))\n"]},{"cell_type":"markdown","source":["Taking the top ten most correlated values gives reg.score of only 0.7910926699369514"],"metadata":{"id":"ulMSOeSS6t9t"},"id":"ulMSOeSS6t9t"},{"cell_type":"code","source":[" def train_model(model, param_grid=[], X=[], y=[], \n","                splits=5, repeats=5):\n","\n","    # get unmodified training data, unless data to use already specified\n","    if len(y)==0:\n","        X,y = get_training_data()\n","    \n","    # create cross-validation method\n","    rkfold = RepeatedKFold(n_splits=splits, n_repeats=repeats)\n","    \n","    # perform a grid search if param_grid given\n","    if len(param_grid)>0:\n","        # setup grid search parameters\n","        gsearch = GridSearchCV(model, param_grid, cv=rkfold,\n","                               scoring=rmse_scorer,\n","                               verbose=1, return_train_score=True)\n","\n","        # search the grid\n","        gsearch.fit(X,y)\n","\n","        # extract best model from the grid\n","        model = gsearch.best_estimator_        \n","        best_idx = gsearch.best_index_\n","\n","        # get cv-scores for best model\n","        grid_results = pd.DataFrame(gsearch.cv_results_)       \n","        cv_mean = abs(grid_results.loc[best_idx,'mean_test_score'])\n","        cv_std = grid_results.loc[best_idx,'std_test_score']\n","\n","    # no grid search, just cross-val score for given model    \n","    else:\n","        grid_results = []\n","        cv_results = cross_val_score(model, X, y, scoring=rmse_scorer, cv=rkfold)\n","        cv_mean = abs(np.mean(cv_results))\n","        cv_std = np.std(cv_results)\n","    \n","    # combine mean and std cv-score in to a pandas series\n","    cv_score = pd.Series({'mean':cv_mean,'std':cv_std})\n","\n","    # predict y using the fitted model\n","    y_pred = model.predict(X)\n","    \n","    # print stats on model performance         \n","    print('----------------------')\n","    print(model)\n","    print('----------------------')\n","    print('score=',model.score(X,y))\n","    print('rmse=',rmse(y, y_pred))\n","    print('cross_val: mean=',cv_mean,', std=',cv_std)\n","\n","\n","\n","# places to store optimal models and scores\n","opt_models = dict()\n","score_models = pd.DataFrame(columns=['mean','std'])\n","\n","# no. k-fold splits\n","splits=5\n","# no. k-fold iterations\n","repeats=5\n","\n","def get_training_data():\n","    # extract training samples\n","    df_train = df\n","    \n","    # split SalePrice and features\n","    y = df_train.SalePrice\n","    X = df_train.drop('SalePrice',axis=1)\n","    \n","    return X, y\n","\n","# metric for evaluation\n","def rmse(y_true, y_pred):\n","    diff = y_pred - y_true\n","    sum_sq = sum(diff**2)    \n","    n = len(y_pred)   \n","    \n","    return np.sqrt(sum_sq/n)\n","\n","# function to detect outliers based on the predictions of a model\n","def find_outliers(model, X, y, sigma=3):\n","\n","    # predict y values using model\n","    try:\n","        y_pred = pd.Series(model.predict(X), index=y.index)\n","    # if predicting fails, try fitting the model first\n","    except:\n","        model.fit(X,y)\n","        y_pred = pd.Series(model.predict(X), index=y.index)\n","        \n","    # calculate residuals between the model prediction and true y values\n","    resid = y - y_pred\n","    mean_resid = resid.mean()\n","    std_resid = resid.std()\n","\n","    # calculate z statistic, define outliers to be where |z|>sigma\n","    z = (resid - mean_resid)/std_resid    \n","    outliers = z[abs(z)>sigma].index\n","    \n","    # print and plot the results\n","    print('R2=',model.score(X,y))\n","    print('rmse=',rmse(y, y_pred))\n","    print('---------------------------------------')\n","\n","    print('mean of residuals:',mean_resid)\n","    print('std of residuals:',std_resid)\n","    print('---------------------------------------')\n","\n","    print(len(outliers),'outliers:')\n","    print(outliers.tolist())\n","\n","# scorer to be used in sklearn model fitting\n","rmse_scorer = make_scorer(rmse, greater_is_better=False)\n","\n"],"metadata":{"id":"-TznCaYARcTn"},"id":"-TznCaYARcTn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = 'Ridge'\n","\n","opt_models[model] = Ridge()\n","alph_range = np.arange(0.25,6,0.25)\n","param_grid = {'alpha': alph_range}\n","\n","opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid)\n","\n","cv_score.name = model\n","score_models = score_models.append(cv_score)\n","\n","plt.figure()\n","plt.errorbar(alph_range, abs(grid_results['mean_test_score']),\n","             abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\n","plt.xlabel('alpha')\n","plt.ylabel('score');"],"metadata":{"id":"hzqN5KycdjZ2"},"id":"hzqN5KycdjZ2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model ='ElasticNet'\n","opt_models[model] = ElasticNet()\n","\n","param_grid = {'alpha': np.arange(1e-4,1e-3,1e-4),\n","              'l1_ratio': np.arange(0.1,1.0,0.1),\n","              'max_iter':[100000]}\n","\n","opt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, splits=splits, repeats=repeats)\n","\n","cv_score.name = model\n","score_models = score_models.append(cv_score)"],"metadata":{"id":"ZVTIASOOXQAq"},"id":"ZVTIASOOXQAq","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"graphs_for_ML_Project.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}